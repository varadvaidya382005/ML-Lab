{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "frCBLSd6IREt"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "from collections import Counter\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVvOnlBSITwb",
        "outputId": "abf56450-4161-423d-f5e8-6fa2e06230a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 781,
          "referenced_widgets": [
            "b5412b25c54542f8a6f4d67f44c67c78",
            "f6765ff3985b4d6ab0daafeb72755184",
            "49ba3b188d794b4b82883112b06eaf30",
            "9604a1e00233452191987641402fd09f",
            "5c749bbb85d847c9a6542b2cb12fb126",
            "97b32a3b279e4bd0802130061ded5c65",
            "f2d2e454e475463aa45601ec735c211a",
            "de8c829cb2bd43fd9cc2734f6888910e",
            "f70fa50a0b554376aed3ee78bed53dc1",
            "fff5d2fb86b7472ea1d7d72b797f8085",
            "8e4dfac4ff1841f8bfcf625a43a126e0",
            "306604331a3e465680ba1fb773816046",
            "3abbc14df152498e9a6d497a6b6e7baa",
            "d56f4a4b20b44b4b842ce8b5385901ae",
            "25ccdc874657485384bb597195275378",
            "edb273b459164723aa0eda2243d8c6db"
          ]
        },
        "id": "zOAKzqUfIcjN",
        "outputId": "337b4203-6739-4c69-ea1a-74d43a314fca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading advanced NLP models...\n",
            "Installing spaCy model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5412b25c54542f8a6f4d67f44c67c78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6765ff3985b4d6ab0daafeb72755184",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49ba3b188d794b4b82883112b06eaf30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/3.52k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9604a1e00233452191987641402fd09f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c749bbb85d847c9a6542b2cb12fb126",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97b32a3b279e4bd0802130061ded5c65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2d2e454e475463aa45601ec735c211a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de8c829cb2bd43fd9cc2734f6888910e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f70fa50a0b554376aed3ee78bed53dc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fff5d2fb86b7472ea1d7d72b797f8085",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e4dfac4ff1841f8bfcf625a43a126e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "306604331a3e465680ba1fb773816046",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3abbc14df152498e9a6d497a6b6e7baa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d56f4a4b20b44b4b842ce8b5385901ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25ccdc874657485384bb597195275378",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edb273b459164723aa0eda2243d8c6db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "# Load advanced models\n",
        "print(\"Loading advanced NLP models...\")\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_md\")  # Medium-sized spaCy model with word vectors\n",
        "except:\n",
        "    print(\"Installing spaCy model...\")\n",
        "    import os\n",
        "    os.system(\"python -m spacy download en_core_web_md\")\n",
        "    nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Load a more powerful sentence transformer model\n",
        "model = SentenceTransformer('paraphrase-mpnet-base-v2')  # More advanced than MiniLM\n",
        "\n",
        "# Load T5-large model for question generation\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
        "\n",
        "asked_questions = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q8ztvslgahX",
        "outputId": "be06614a-f020-4040-ed93-1d7332758e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ§  Socrateach AI Advanced - Conceptual Learning Through Questions\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "def extract_key_entities(text):\n",
        "    \"\"\"Extract important entities and concepts using spaCy.\"\"\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract named entities\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Extract noun chunks (important phrases)\n",
        "    noun_chunks = [chunk.text for chunk in doc.noun_chunks if not all(token.is_stop for token in chunk)]\n",
        "\n",
        "    # Extract key words based on dependency parsing\n",
        "    subjects = [token.text for token in doc if token.dep_ in ('nsubj', 'nsubjpass')]\n",
        "    objects = [token.text for token in doc if token.dep_ in ('dobj', 'pobj', 'attr')]\n",
        "\n",
        "    # Get keywords using statistical importance\n",
        "    keywords = [token.text for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
        "    keyword_freq = Counter(keywords)\n",
        "    important_keywords = [word for word, count in keyword_freq.most_common(5)]\n",
        "\n",
        "    return {\n",
        "        'entities': entities,\n",
        "        'noun_chunks': noun_chunks,\n",
        "        'subjects': subjects,\n",
        "        'objects': objects,\n",
        "        'keywords': important_keywords\n",
        "    }\n",
        "\n",
        "def generate_t5_questions(context, num_questions=3):\n",
        "    \"\"\"Generate questions using T5 large model.\"\"\"\n",
        "    input_text = \"generate questions: \" + context\n",
        "\n",
        "    # Tokenize and generate\n",
        "    input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    outputs = t5_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=100,\n",
        "        num_return_sequences=num_questions,\n",
        "        num_beams=8,\n",
        "        temperature=1.0,\n",
        "        top_k=100,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode and clean up the generated questions\n",
        "    questions = []\n",
        "    for output in outputs:\n",
        "        question = t5_tokenizer.decode(output, skip_special_tokens=True)\n",
        "        # T5 sometimes generates \"question: \" prefix\n",
        "        question = question.replace(\"question: \", \"\").strip()\n",
        "        if question and question not in questions and question.endswith(\"?\"):\n",
        "            questions.append(question)\n",
        "\n",
        "    return questions\n",
        "\n",
        "def is_definition_sentence(sentence):\n",
        "    \"\"\"Check if a sentence contains a definition pattern.\"\"\"\n",
        "    definition_patterns = [\n",
        "        \"is defined as\", \"refers to\", \"means\", \"is a\", \"are\", \"represents\",\n",
        "        \"consists of\", \"is characterized by\", \"is known as\", \"can be described as\"\n",
        "    ]\n",
        "    return any(pattern in sentence.lower() for pattern in definition_patterns)\n",
        "\n",
        "def generate_theory_questions(chunk):\n",
        "    \"\"\"Generate questions focused on theoretical knowledge and definitions.\"\"\"\n",
        "    doc = nlp(chunk)\n",
        "    questions = []\n",
        "\n",
        "    # Look for potential definition sentences and key terms\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text\n",
        "\n",
        "        if is_definition_sentence(sent_text):\n",
        "            # Extract the term being defined\n",
        "            for noun_chunk in nlp(sent_text).noun_chunks:\n",
        "                if len(noun_chunk.text.split()) <= 3 and not all(token.is_stop for token in noun_chunk):\n",
        "                    term = noun_chunk.text\n",
        "                    theory_questions = [\n",
        "                        f\"What is {term}?\",\n",
        "                        f\"Define {term} in your own words.\",\n",
        "                        f\"Explain the concept of {term}.\"\n",
        "                    ]\n",
        "                    questions.append(random.choice(theory_questions))\n",
        "                    break\n",
        "\n",
        "    # Extract key entities and create questions about them\n",
        "    key_info = extract_key_entities(chunk)\n",
        "    entities = [e[0] for e in key_info['entities']]\n",
        "\n",
        "    if entities:\n",
        "        for entity in entities[:2]:  # Limit to avoid too many similar questions\n",
        "            if len(entity.split()) <= 3:  # Keep it concise\n",
        "                theory_questions = [\n",
        "                    f\"What is the significance of {entity}?\",\n",
        "                    f\"Describe {entity} according to the text.\",\n",
        "                    f\"Explain the role of {entity} in this context.\"\n",
        "                ]\n",
        "                questions.append(random.choice(theory_questions))\n",
        "\n",
        "    # Check for processes or methods in the text\n",
        "    process_indicators = [\"process\", \"method\", \"procedure\", \"technique\", \"approach\", \"steps\"]\n",
        "    for indicator in process_indicators:\n",
        "        if indicator in chunk.lower():\n",
        "            questions.append(f\"Describe the {indicator} mentioned in the text.\")\n",
        "            break\n",
        "\n",
        "    return questions[:2]  # Limit to 2 theory questions per chunk\n",
        "\n",
        "def generate_advanced_questions(chapter_text):\n",
        "    \"\"\"Generate high-quality questions with both conceptual and theoretical focus.\"\"\"\n",
        "    sentences = sent_tokenize(chapter_text)\n",
        "    questions = []\n",
        "\n",
        "    # Extract global concepts and themes first\n",
        "    doc = nlp(chapter_text)\n",
        "    global_entities = extract_key_entities(chapter_text)\n",
        "    global_concepts = set(global_entities['keywords'] + [e[0] for e in global_entities['entities']])\n",
        "\n",
        "    # Get main concepts from noun chunks\n",
        "    main_concepts = []\n",
        "    for chunk in doc.noun_chunks:\n",
        "        if not all(token.is_stop for token in chunk) and len(chunk.text.split()) <= 3:\n",
        "            main_concepts.append(chunk.text)\n",
        "\n",
        "    if not main_concepts and global_concepts:\n",
        "        main_concepts = list(global_concepts)[:5]\n",
        "\n",
        "    # Process larger chunks for better context\n",
        "    chunk_size = 5  # Larger chunk size for better context\n",
        "    for i in range(0, len(sentences), chunk_size):\n",
        "        chunk = \" \".join(sentences[i:i+chunk_size])\n",
        "\n",
        "        # Skip very short chunks\n",
        "        if len(chunk.split()) < 20:\n",
        "            continue\n",
        "\n",
        "        # 1. Generate THEORY questions (50% of questions)\n",
        "        theory_questions = generate_theory_questions(chunk)\n",
        "        for question in theory_questions:\n",
        "            questions.append((question, chunk))\n",
        "\n",
        "        # 2. Generate T5 questions (25% of questions)\n",
        "        t5_questions = generate_t5_questions(chunk, num_questions=1)\n",
        "        for question in t5_questions:\n",
        "            # Check if it's not a simple factual question\n",
        "            if not question.lower().startswith((\"what is\", \"who is\", \"when did\")):\n",
        "                questions.append((question, chunk))\n",
        "\n",
        "        # 3. Generate CONCEPTUAL questions (25% of questions)\n",
        "        if main_concepts:\n",
        "            # Connection questions between concepts\n",
        "            if len(main_concepts) >= 2:\n",
        "                concept1 = random.choice(main_concepts)\n",
        "                concept2 = random.choice([c for c in main_concepts if c != concept1])\n",
        "                conceptual_questions = [\n",
        "                    f\"How does {concept1} relate to {concept2}?\",\n",
        "                    f\"Compare and contrast {concept1} and {concept2}.\",\n",
        "                    f\"Explain the relationship between {concept1} and {concept2} in your own words.\"\n",
        "                ]\n",
        "                questions.append((random.choice(conceptual_questions), chunk))\n",
        "\n",
        "            # Application questions\n",
        "            concept = random.choice(main_concepts)\n",
        "            application_questions = [\n",
        "                f\"How would you apply the concept of {concept} in a real-world situation?\",\n",
        "                f\"What are the practical implications of {concept}?\",\n",
        "                f\"How might {concept} be used to solve a problem?\"\n",
        "            ]\n",
        "            questions.append((random.choice(application_questions), chunk))\n",
        "\n",
        "    # Add some high-level conceptual questions about the entire text\n",
        "    if len(chapter_text.split()) > 100:\n",
        "        bloom_questions = [\n",
        "            \"What are the core principles presented in this material?\",\n",
        "            \"How would you synthesize the main concepts from this text?\",\n",
        "            \"What real-world applications can you derive from these concepts?\",\n",
        "            \"How would you explain these concepts to someone with no background in this field?\",\n",
        "            \"What are the most significant insights you can draw from this material?\"\n",
        "        ]\n",
        "\n",
        "        for q in bloom_questions[:2]:  # Add a couple higher-order questions\n",
        "            questions.append((q, chapter_text))\n",
        "\n",
        "    return questions\n",
        "\n",
        "def compute_tfidf_similarity(user_answer, context):\n",
        "    \"\"\"Calculate TF-IDF based similarity between answer and context.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform([user_answer, context])\n",
        "        return util.pytorch_cos_sim(\n",
        "            torch.tensor(tfidf_matrix[0].toarray()),\n",
        "            torch.tensor(tfidf_matrix[1].toarray())\n",
        "        ).item()\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def evaluate_answer_quality(user_answer, context_text, threshold=0.6):\n",
        "    \"\"\"Evaluate answer based on conceptual understanding rather than text matching.\"\"\"\n",
        "    if not user_answer.strip():\n",
        "        return False, 0.0, \"No answer provided.\"\n",
        "\n",
        "    # Normalize texts\n",
        "    user_answer = re.sub(r'\\s+', ' ', user_answer.strip().lower())\n",
        "    context_text = re.sub(r'\\s+', ' ', context_text.strip().lower())\n",
        "\n",
        "    # Process with spaCy for concept extraction\n",
        "    answer_doc = nlp(user_answer)\n",
        "    context_doc = nlp(context_text)\n",
        "\n",
        "    # FOCUS ON SEMANTIC SIMILARITY (60%) - most important for conceptual evaluation\n",
        "    embed_answer = model.encode(user_answer, convert_to_tensor=True)\n",
        "    embed_context = model.encode(context_text, convert_to_tensor=True)\n",
        "    semantic_sim = util.pytorch_cos_sim(embed_answer, embed_context).item()\n",
        "\n",
        "    # Extract concepts from both texts using improved method\n",
        "    context_concepts = set()\n",
        "    for chunk in context_doc.noun_chunks:\n",
        "        if not all(token.is_stop for token in chunk) and len(chunk.text) > 3:\n",
        "            context_concepts.add(chunk.text.lower())\n",
        "\n",
        "    # Add entities to concepts\n",
        "    for ent in context_doc.ents:\n",
        "        context_concepts.add(ent.text.lower())\n",
        "\n",
        "    answer_concepts = set()\n",
        "    for chunk in answer_doc.noun_chunks:\n",
        "        if not all(token.is_stop for token in chunk) and len(chunk.text) > 3:\n",
        "            answer_concepts.add(chunk.text.lower())\n",
        "\n",
        "    for ent in answer_doc.ents:\n",
        "        answer_concepts.add(ent.text.lower())\n",
        "\n",
        "    # Concept matching with approximate matching (20%)\n",
        "    concept_matches = 0\n",
        "    if context_concepts:\n",
        "        for ans_concept in answer_concepts:\n",
        "            ans_embed = model.encode(ans_concept, convert_to_tensor=True)\n",
        "\n",
        "            # Find the best matching context concept\n",
        "            max_sim = 0\n",
        "            for ctx_concept in context_concepts:\n",
        "                ctx_embed = model.encode(ctx_concept, convert_to_tensor=True)\n",
        "                sim = util.pytorch_cos_sim(ans_embed, ctx_embed).item()\n",
        "                max_sim = max(max_sim, sim)\n",
        "\n",
        "            # Count as match if similarity is high enough\n",
        "            if max_sim > 0.6:\n",
        "                concept_matches += 1\n",
        "\n",
        "        # Calculate concept coverage score\n",
        "        concept_coverage = min(1.0, concept_matches / max(3, len(context_concepts) * 0.3))\n",
        "    else:\n",
        "        concept_coverage = 0.5  # Neutral score if no concepts found\n",
        "\n",
        "    # Check answer quality (20%)\n",
        "    # Length adequacy (not too short)\n",
        "    min_expected_tokens = 15\n",
        "    length_score = min(1.0, len(answer_doc) / min_expected_tokens)\n",
        "\n",
        "    # Check for at least one complete sentence\n",
        "    has_sentence = any(sent.text.strip() for sent in answer_doc.sents)\n",
        "    sentence_score = 1.0 if has_sentence else 0.3\n",
        "\n",
        "    quality_score = (0.7 * length_score + 0.3 * sentence_score)\n",
        "\n",
        "    # Calculate final score with emphasis on meaning over matching\n",
        "    final_score = (\n",
        "        0.60 * semantic_sim +      # Higher weight for semantic understanding\n",
        "        0.20 * concept_coverage +  # Some weight for concept coverage\n",
        "        0.20 * quality_score       # Some weight for answer quality\n",
        "    )\n",
        "\n",
        "    # Dynamic threshold adjustment for question type\n",
        "    # Theory questions may need more concept coverage, so make it a bit stricter\n",
        "    if any(term in context_text.lower() for term in [\"define\", \"what is\", \"explain the\"]):\n",
        "        adjusted_threshold = threshold + 0.05\n",
        "    else:\n",
        "        adjusted_threshold = threshold\n",
        "\n",
        "    # Generate helpful, specific feedback\n",
        "    if final_score >= adjusted_threshold:\n",
        "        if semantic_sim > 0.7:\n",
        "            feedback = \"Excellent! Your answer shows strong conceptual understanding.\"\n",
        "        else:\n",
        "            feedback = \"Good answer. You've covered the key concepts well.\"\n",
        "    elif final_score >= adjusted_threshold * 0.7:\n",
        "        feedback = \"Your answer shows partial understanding of the concepts.\"\n",
        "        if concept_coverage < 0.4:\n",
        "            feedback += \" Try to include more key terminology from the material.\"\n",
        "        if semantic_sim < 0.5:\n",
        "            feedback += \" Make sure your explanation aligns with the core ideas in the text.\"\n",
        "    else:\n",
        "        feedback = \"Your answer needs improvement in conceptual understanding.\"\n",
        "        if concept_coverage < 0.3:\n",
        "            # Provide specific concepts they missed\n",
        "            missed_concepts = list(context_concepts)[:3]\n",
        "            feedback += f\" Consider discussing concepts like: {', '.join(missed_concepts)}.\"\n",
        "        if quality_score < 0.4:\n",
        "            feedback += \" Try to provide a more complete explanation.\"\n",
        "\n",
        "    return final_score >= adjusted_threshold, final_score, feedback\n",
        "\n",
        "def extract_teaching_points(context):\n",
        "    \"\"\"Extract true conceptual teaching points, not just text fragments.\"\"\"\n",
        "    doc = nlp(context)\n",
        "\n",
        "    # Focus on extracting real concepts\n",
        "    teaching_points = []\n",
        "\n",
        "    # 1. Look for definition sentences - these often contain key concepts\n",
        "    definition_sentences = []\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text\n",
        "        if is_definition_sentence(sent_text) and len(sent_text.split()) <= 25:\n",
        "            definition_sentences.append(sent_text)\n",
        "\n",
        "    # 2. Extract key terms and their contexts\n",
        "    key_terms = []\n",
        "    for chunk in doc.noun_chunks:\n",
        "        if not all(token.is_stop for token in chunk) and 2 <= len(chunk.text.split()) <= 4:\n",
        "            # Find a sentence containing this term\n",
        "            for sent in doc.sents:\n",
        "                if chunk.text in sent.text and len(sent.text.split()) <= 25:\n",
        "                    key_terms.append(sent.text)\n",
        "                    break\n",
        "\n",
        "    # 3. Find sentences with high information density\n",
        "    info_sentences = []\n",
        "    for sent in doc.sents:\n",
        "        # Count entities, important nouns, and verbs\n",
        "        sent_doc = nlp(sent.text)\n",
        "        entity_count = len(sent_doc.ents)\n",
        "        noun_count = len([token for token in sent_doc if token.pos_ == \"NOUN\" and not token.is_stop])\n",
        "        verb_count = len([token for token in sent_doc if token.pos_ == \"VERB\" and not token.is_stop])\n",
        "\n",
        "        # Higher score means more informative\n",
        "        info_score = entity_count + 0.5 * noun_count + 0.3 * verb_count\n",
        "\n",
        "        if info_score >= 3 and len(sent.text.split()) <= 25:\n",
        "            info_sentences.append((sent.text, info_score))\n",
        "\n",
        "    # Sort by information score\n",
        "    info_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_info = [s[0] for s in info_sentences[:2]]\n",
        "\n",
        "    # Combine all sources, prioritizing definitions\n",
        "    teaching_points = definition_sentences[:2] + key_terms[:2] + top_info\n",
        "\n",
        "    # Ensure no duplicates\n",
        "    unique_points = []\n",
        "    for point in teaching_points:\n",
        "        # Check if this point is too similar to existing ones\n",
        "        if not any(util.pytorch_cos_sim(\n",
        "            model.encode(point, convert_to_tensor=True),\n",
        "            model.encode(existing, convert_to_tensor=True)\n",
        "        ).item() > 0.8 for existing in unique_points):\n",
        "            unique_points.append(point)\n",
        "            if len(unique_points) >= 4:  # Limit to 4 points\n",
        "                break\n",
        "\n",
        "    # If we couldn't extract enough points, create synthetic ones\n",
        "    if len(unique_points) < 2:\n",
        "        # Extract key entities and concepts\n",
        "        key_info = extract_key_entities(context)\n",
        "        key_terms = key_info['keywords'][:3]\n",
        "\n",
        "        for term in key_terms:\n",
        "            unique_points.append(f\"The concept of {term} is central to understanding this material.\")\n",
        "\n",
        "    return unique_points[:4]  # Return up to 4 teaching points\n",
        "\n",
        "def socrateach():\n",
        "    print(\"\\nðŸ§  Socrateach AI Advanced - Conceptual Learning Through Questions\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    chapter_text = input(\"\\nðŸ“š Paste your chapter text here:\\n\")\n",
        "\n",
        "    if len(chapter_text.split()) < 30:\n",
        "        print(\"âš ï¸ Text is too short. Please provide a longer passage (at least 30 words).\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nâ³ Analyzing text and generating conceptual and theory questions...\")\n",
        "    all_questions = generate_advanced_questions(chapter_text)\n",
        "\n",
        "    if not all_questions:\n",
        "        print(\"âš ï¸ Could not generate meaningful questions. Please try a different text.\")\n",
        "        return\n",
        "\n",
        "    print(f\"ðŸ“ Generated {len(all_questions)} insightful questions from your text.\")\n",
        "\n",
        "    # Filter to remove duplicates and similar questions\n",
        "    filtered_questions = []\n",
        "    question_embeddings = []\n",
        "\n",
        "    for q, context in all_questions:\n",
        "        q_embed = model.encode(q, convert_to_tensor=True)\n",
        "\n",
        "        # Check if similar to existing questions\n",
        "        is_duplicate = False\n",
        "        for existing_embed in question_embeddings:\n",
        "            similarity = util.pytorch_cos_sim(q_embed, existing_embed).item()\n",
        "            if similarity > 0.7:  # Adjusted threshold for diversity\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            filtered_questions.append((q, context))\n",
        "            question_embeddings.append(q_embed)\n",
        "\n",
        "    print(f\"ðŸ” Filtered to {len(filtered_questions)} unique questions.\")\n",
        "\n",
        "    # Option to export question bank\n",
        "    export = input(\"\\nWould you like to export the complete question bank? (yes/no): \").lower()\n",
        "    if export.startswith('y'):\n",
        "        with open(\"question_bank.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"GENERATED QUESTION BANK\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "            for i, (q, _) in enumerate(filtered_questions):\n",
        "                f.write(f\"Question {i+1}: {q}\\n\\n\")\n",
        "        print(\"âœ… Question bank exported to 'question_bank.txt'\")\n",
        "\n",
        "    print(\"\\nðŸ§  Beginning your conceptual learning session...\")\n",
        "\n",
        "    # Mix of theory and conceptual questions\n",
        "    # Sort to put theory questions first, then conceptual\n",
        "    filtered_questions.sort(key=lambda x: 0 if any(term in x[0].lower() for term in [\"what is\", \"define\", \"describe\"]) else 1)\n",
        "\n",
        "    question_count = min(len(filtered_questions), 5)\n",
        "    correct_count = 0\n",
        "    scores = []\n",
        "\n",
        "    for i in range(question_count):\n",
        "        question, context = filtered_questions[i]\n",
        "\n",
        "        print(f\"\\nâ“ Question {i+1}/{question_count}: {question}\")\n",
        "        user_answer = input(\"ðŸ’¬ Your answer: \").strip()\n",
        "\n",
        "        is_correct, score, feedback = evaluate_answer_quality(user_answer, context)\n",
        "        scores.append(score)\n",
        "\n",
        "        # Provide appropriate feedback\n",
        "        if is_correct:\n",
        "            print(f\"âœ… Excellent understanding! ({score:.2f} concept mastery score)\")\n",
        "            print(f\"ðŸ“Š {feedback}\")\n",
        "            correct_count += 1\n",
        "        elif score >= 0.5:\n",
        "            print(f\"ðŸŸ¨ Partial understanding. ({score:.2f} concept mastery score)\")\n",
        "            print(f\"ðŸ“Š {feedback}\")\n",
        "\n",
        "            teaching_points = extract_teaching_points(context)\n",
        "            print(\"\\nðŸ“Œ Key concepts to reinforce:\")\n",
        "            for j, point in enumerate(teaching_points):\n",
        "                print(f\"  {j+1}. {point}\")\n",
        "        else:\n",
        "            print(f\"âŒ Needs improvement. ({score:.2f} concept mastery score)\")\n",
        "            print(f\"ðŸ“Š {feedback}\")\n",
        "\n",
        "            teaching_points = extract_teaching_points(context)\n",
        "            print(\"\\nðŸ“Œ Key concepts to understand:\")\n",
        "            for j, point in enumerate(teaching_points):\n",
        "                print(f\"  {j+1}. {point}\")\n",
        "\n",
        "    # Detailed session analysis\n",
        "    avg_score = sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"ðŸŽ“ Session complete! Overall concept mastery: {avg_score:.2f}\")\n",
        "    print(f\"ðŸ“Š You demonstrated good understanding on {correct_count} out of {question_count} questions.\")\n",
        "\n",
        "    # Personalized feedback\n",
        "    if avg_score >= 0.85:\n",
        "        print(\"ðŸŒŸ Outstanding! You've demonstrated excellent conceptual mastery of the material.\")\n",
        "    elif avg_score >= 0.7:\n",
        "        print(\"ðŸŽ‰ Great work! You have a solid grasp of the key concepts.\")\n",
        "    elif avg_score >= 0.5:\n",
        "        print(\"ðŸ‘ Good progress! With more focus on the core concepts, you'll master this material.\")\n",
        "    else:\n",
        "        print(\"ðŸ“š This topic needs more review. Focus on understanding the fundamental concepts.\")\n",
        "\n",
        "    # Areas for improvement based on questions with lowest scores\n",
        "    if scores:\n",
        "        worst_q_index = scores.index(min(scores))\n",
        "        worst_q, worst_context = filtered_questions[worst_q_index]\n",
        "\n",
        "        print(f\"\\nðŸ“ Concept to focus on: '{worst_q}'\")\n",
        "        teaching_points = extract_teaching_points(worst_context)\n",
        "        print(\"Review these key points:\")\n",
        "        for j, point in enumerate(teaching_points):\n",
        "            print(f\"  {j+1}. {point}\")\n",
        "\n",
        "    # Ask if they want to continue\n",
        "    if len(filtered_questions) > question_count:\n",
        "        more = input(\"\\nWould you like to continue with more questions? (yes/no): \").lower()\n",
        "        if more.startswith('y'):\n",
        "            print(\"\\nContinuing with more questions...\\n\")\n",
        "            remaining_questions = filtered_questions[question_count:]\n",
        "            filtered_questions = remaining_questions\n",
        "            # Continue with the next batch\n",
        "            socrateach()\n",
        "    else:\n",
        "        print(\"\\nâœ¨ You've completed all available questions for this text!\")\n",
        "        print(\"ðŸ“š For more practice, try with a different section of your study materials.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    socrateach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m7wKwZ4gdr1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}